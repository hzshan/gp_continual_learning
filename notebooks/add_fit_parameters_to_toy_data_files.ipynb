{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load result files from toy-model results, fit various parameters of long-term forgetting, and then attach them to the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Generating a list at 2023-05-27 22:25\n",
      "[0] gp_toy_30x50_xsim80_1L_10context_diff_strength\n",
      "[1] gp_toy_30x50_xsim90_1L_50context_diff_strength\n",
      "[2] gp_toy_30x50_xsim60_1L_50context_diff_strength\n",
      "[3] gp_toy_30x50_xsim30_1L_50context_diff_strength\n",
      "[4] gp_toy_30x50_xsim80_1L_50context_diff_strength\n",
      "[5] gp_toy_30x50_xsim60_1L_10context_diff_strength\n",
      "[6] gp_toy_30x50_xsim0_1L_50context_diff_strength\n",
      "No key was specified. Automatically using key <<context_strength>> to sort the results.\n",
      "\"NSEEDS\" found in the arguments. Assuming that each file contains multiple random seeds.\n",
      "=================== Cluster organizer ===================\n",
      "11 data objects loaded from folder \"gp_toy_30x50_xsim90_1L_50context_diff_strength\".\n",
      "For key <<context_strength>>, the values are [35.0, 30.0, 20.0, 25.0, 0.0, 45.0, 40.0, 5.0, 50.0, 15.0, 10.0]\n",
      "=================== Cluster organizer ===================\n",
      "Available data keys are dict_keys(['args', 'train loss', 'test loss', 'train acc', 'test acc', 'train loss naive', 'test loss naive', 'train acc naive', 'test acc naive', 'train magnitude', 'train magnitude naive', 'tr(P1P2)/P', 'V1-V2'])\n",
      "dict_keys(['args', 'train loss', 'test loss', 'train acc', 'test acc', 'train loss naive', 'test loss naive', 'train acc naive', 'test acc naive', 'train magnitude', 'train magnitude naive', 'tr(P1P2)/P', 'V1-V2'])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('/Users/haozheshan/Dropbox/codes/gp_continual_learning/')\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import theory, cluster_utils, torch, data, utils, pickle\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# USE KEYWORDS BELOW TO SEARCH FOR FOLDERS\n",
    "batch_name_list =cluster_utils.list_folders('cluster_results/', 'diff_strength', 'toy')\n",
    "\n",
    "#################\n",
    "folder_index = 1\n",
    "#################\n",
    "organizer = cluster_utils.ClusterResultOrganizer('cluster_results/', batch_name=batch_name_list[folder_index], sort_by_key=None)\n",
    "all_train_loss = organizer.organize_results('train loss')\n",
    "all_train_mag = organizer.organize_results('train magnitude')\n",
    "\n",
    "print(organizer.all_data_obj[0].keys())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The cell below does an exponential fit to the loss curves and magnitude curves and compute two forgetting OPs. The results are saved back into the pickle files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in toy-model results, each file contains all random seeds corresponding to the same set of hyperparameters\n",
    "for file_ind in range(len(organizer.all_data_obj)):\n",
    "\n",
    "    # fix exponential relaxation to M and A dynamics\n",
    "    L1 = organizer.all_data_obj[file_ind]['train loss'].mean(0)[0]\n",
    "    M1 = np.array(organizer.all_data_obj[file_ind]['train magnitude']).mean(0)[0]\n",
    "    A1 = -0.5 * (L1 - M1 - 1)\n",
    "    m_asym, m_tau, _ = utils.exponential_fit(np.arange(len(M1)), M1)\n",
    "    a_asym, a_tau, _ = utils.exponential_fit(np.arange(len(M1)), A1)\n",
    "\n",
    "    # recreate the input data\n",
    "    _args = organizer.all_data_obj[file_ind]['args']\n",
    "\n",
    "    all_trP1P2 = []\n",
    "    all_V1_minus_V2 = []\n",
    "\n",
    "    for seed in range(_args.NSEEDS):\n",
    "        torch.manual_seed(seed)\n",
    "\n",
    "        seq_of_train_x, seq_of_test_x, seq_of_train_y, _ =\\\n",
    "            data.prepare_cluster_dataset(num_tasks=2,\n",
    "                                        train_p=_args.P,\n",
    "                                        test_p=2,\n",
    "                                        num_clusters=_args.NC,\n",
    "                                        input_dim=_args.N0,\n",
    "                                        hidden_dim=_args.Nh,\n",
    "                                        relative_radius=0.1,\n",
    "                                        teacher_similarity=_args.tsim,\n",
    "                                        input_similarity=_args.xsim,\n",
    "                                        accumulate=False,\n",
    "                                        precision=64)\n",
    "        \n",
    "        seq_of_train_x, seq_of_test_x = data.add_task_embedding(seq_of_train_x, seq_of_test_x, _args.N0context, _args.context_strength)\n",
    "\n",
    "        k1 = theory.k_ntk(seq_of_train_x[0], seq_of_train_x[0], depth=_args.depth)\n",
    "        k2 = theory.k_ntk(seq_of_train_x[1], seq_of_train_x[1], depth=_args.depth)\n",
    "        k12 = theory.k_ntk(seq_of_train_x[0], seq_of_train_x[1], depth=_args.depth)\n",
    "        k1_inv = torch.inverse(k1)\n",
    "        k2_inv = torch.inverse(k2)\n",
    "\n",
    "        y1 = seq_of_train_y[0]\n",
    "        y2 = seq_of_train_y[1]\n",
    "\n",
    "        all_trP1P2.append(float(torch.trace(k1_inv @ k12 @ k2_inv @ k12.T) / _args.P))\n",
    "        all_V1_minus_V2.append(2 - 2 * float(y1.T @ k1_inv @ k12 @ k2_inv @ y2) / float((y1.T @ k1_inv @ y1)))\n",
    "    \n",
    "    all_trP1P2 = np.array(all_trP1P2)\n",
    "    all_V1_minus_V2 = np.array(all_V1_minus_V2)\n",
    "\n",
    "    # save the new data\n",
    "    organizer.all_data_obj[file_ind]['M tau'] = m_tau\n",
    "    organizer.all_data_obj[file_ind]['A tau'] = a_tau\n",
    "    organizer.all_data_obj[file_ind]['M asym'] = m_asym\n",
    "    organizer.all_data_obj[file_ind]['A asym'] = a_asym\n",
    "    organizer.all_data_obj[file_ind]['tr(P1P2)/P'] = all_trP1P2\n",
    "    organizer.all_data_obj[file_ind]['V1-V2'] = all_V1_minus_V2\n",
    "    \n",
    "    pickle.dump(organizer.all_data_obj[file_ind], open(organizer.file_path + organizer.file_name_list[file_ind], 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(organizer.file_name_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
